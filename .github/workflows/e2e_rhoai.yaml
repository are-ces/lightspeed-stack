# .github/workflows/e2e_rhoai.yml
name:  RHOAI E2E Tests

on: 
  push

jobs:
  e2e_rhoai:
    runs-on: ubuntu-latest
    env:
      AWS_DEFAULT_REGION: "us-east-1"
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      PULL_SECRET: ${{ secrets.PULL_SECRET }}
      SSH_PUBLIC_KEY: ${{ secrets.SSH_PUBLIC_KEY }}
      BASE_DOMAIN: ${{ secrets.BASE_DOMAIN }}
      CLUSTER_NAME: ${{ secrets.CLUSTER_NAME }}
      OPENSHIFT_WORKDIR: "./ocp-cluster"
      VLLM_API_KEY: ${{ secrets.VLLM_API_KEY }}
      HUGGING_FACE_HUB_TOKEN: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}

    steps:
      - uses: actions/checkout@v4
        with:
          # On PR_TARGET → the fork (or same repo) that opened the PR.
          # On push      → falls back to the current repository.
          repository: ${{ github.event.pull_request.head.repo.full_name || github.repository }}

          # On PR_TARGET → the PR head *commit* (reproducible).
          # On push      → the pushed commit that triggered the workflow.
          ref: ${{ github.event.pull_request.head.ref || github.sha }}

          # Don’t keep credentials when running untrusted PR code under PR_TARGET.
          persist-credentials: ${{ github.event_name != 'pull_request_target' }}

      - name: Debug checkout for umago/lightspeed-stack setup-metrics branch
        run: |
          echo "=== GitHub Event Information ==="
          echo "Event name: ${{ github.event_name }}"
          echo "Base repo: ${{ github.repository }}"
          echo "Base SHA: ${{ github.sha }}"
          echo ""
          echo "=== PR Information ==="
          echo "PR head repo: '${{ github.event.pull_request.head.repo.full_name }}'"
          echo "PR head ref: '${{ github.event.pull_request.head.ref }}'"
          echo "PR head SHA: '${{ github.event.pull_request.head.sha }}'"
          echo "PR number: ${{ github.event.pull_request.number }}"
          echo ""
          echo "=== Resolved Checkout Values ==="
          echo "Repository used: ${{ github.event.pull_request.head.repo.full_name || github.repository }}"
          echo "Ref used: ${{ github.event.pull_request.head.ref || github.sha }}"
          echo ""
          echo "=== Expected for umago/lightspeed-stack:setup-metrics ==="
          echo "Should be repo: umago/lightspeed-stack" 
          echo "Should be ref: setup-metrics"

      - name: Verify actual git checkout result
        run: |
          echo "=== Git Status After Checkout ==="
          echo "Remote URLs:"
          git remote -v
          echo ""
          echo "Current branch: $(git branch --show-current 2>/dev/null || echo 'detached HEAD')"
          echo "Current commit: $(git rev-parse HEAD)"
          echo "Current commit message: $(git log -1 --oneline)"
          echo ""
          echo "=== Recent commits (should show setup-metrics commits) ==="
          git log --oneline -5

      - uses: 1arp/create-a-file-action@0.4.5
        with:
          path: '.'
          isAbsolutePath: false
          file: 'lightspeed-stack.yaml'
          content: |
            name: Lightspeed Core Service (LCS)
            service:
              host: 0.0.0.0
              port: 8080
              auth_enabled: false
              workers: 1
              color_log: true
              access_log: true
            llama_stack:
              # Uses a remote llama-stack service
              # The instance would have already been started with a llama-stack-run.yaml file
              use_as_library_client: false
              # Alternative for "as library use"
              # use_as_library_client: true
              # library_client_config_path: <path-to-llama-stack-run.yaml-file>
              url: http://llama-stack:8321
              api_key: xyzzy
            user_data_collection:
              feedback_enabled: true
              feedback_storage: "/tmp/data/feedback"
              transcripts_enabled: true
              transcripts_storage: "/tmp/data/transcripts"

            authentication:
              module: "noop"
              
      # - name: Set up SSH
      #   run: |
      #     mkdir -p ~/.ssh
      #     echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_ed25519
      #     chmod 600 ~/.ssh/id_ed25519

      - name: list files
        run: |
          ls
          cat lightspeed-stack.yaml
          cat run.yaml

      - name: Download OpenShift installer
        run: |
          mkdir -p $HOME/bin
          curl -L -o openshift-install-linux.tar.gz "https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-install-linux.tar.gz"
          tar -xzf openshift-install-linux.tar.gz
          rm openshift-install-linux.tar.gz
          mv openshift-install $HOME/bin/
          chmod +x $HOME/bin/openshift-install

          echo "$HOME/bin" >> $GITHUB_PATH

      - name: Verify OpenShift installer
        run: |
          if ! command -v openshift-install &> /dev/null
          then
            echo "❌ openshift-install not found. Installation failed."
            exit 1
          fi

          echo "✅ openshift-install is installed."
          openshift-install version

      - name: Install OpenShift client (oc)
        run: |
          curl -L -o openshift-client-linux.tar.gz "https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-client-linux.tar.gz"
          tar -xzf openshift-client-linux.tar.gz
          rm openshift-client-linux.tar.gz
          mv oc kubectl $HOME/bin/
          chmod +x $HOME/bin/oc $HOME/bin/kubectl
          echo "$HOME/bin" >> $GITHUB_PATH

      - name: Verify OpenShift client installation
        run: |
          if ! command -v oc &> /dev/null
          then
            echo "❌ oc CLI not found. Installation failed."
            exit 1
          fi

          echo "✅ oc CLI is installed."
          oc version


      - name: list files 2
        run: |
          ls

      - name: Deploy OpenShift cluster
        run: |
          ./e2e/RHOAI/deploy_openshift_aws.sh

      - name: Set KUBECONFIG for subsequent steps
        run: |
          echo "KUBECONFIG=$OPENSHIFT_WORKDIR/auth/kubeconfig" >> $GITHUB_ENV

      - name: Create Hugging Face secret
        run: |
          oc create secret generic hf-token-secret \
            --from-literal=token="${{ secrets.HUGGING_FACE_HUB_TOKEN }}" \
            -n default || echo "Secret exists"

      - name: Create vLLM API key secret
        run: |
          oc create secret generic vllm-api-key-secret \
            --from-literal=key="${{ secrets.VLLM_API_KEY }}" \
            -n default || echo "Secret exists"

      - name: Download tool calling config
        run: |
          curl -L -o tool_chat_template_llama3.2_json.jinja \
          https://raw.githubusercontent.com/vllm-project/vllm/main/examples/tool_chat_template_llama3.2_json.jinja
          
          oc create configmap vllm-chat-template \
          --from-file=tool_chat_template_llama3.2_json.jinja


      - name: Deploy pod
        run: |
          ./e2e/RHOAI/pipeline.sh

      - name: Expose service
        run: |
          ./e2e/RHOAI/scripts/expose-service.sh

      - name: list files 3
        run: |
          ls
          cat pod.env
          oc get pods

      - name: Call vLLM API
        run: |
          ./e2e/RHOAI/scripts/infer.sh "${{ secrets.VLLM_API_KEY }}"

      - name: Save vLLM URL
        run: |
          source pod.env
          echo "Using the computed URL: $KSVC_URL"
          echo "KSVC_URL=$KSVC_URL" >> $GITHUB_ENV

      - uses: 1arp/create-a-file-action@0.4.5
        env: 
            KSVC_URL: ${{ env.KSVC_URL }}
            VLLM_API_KEY: ${{ secrets.VLLM_API_KEY }}
        with:
          path: '.'
          isAbsolutePath: false
          file: 'run.yaml'
          content: |
            version: 2
            image_name: rag-configuration-demo3

            apis:
            - agents
            - inference
            - vector_io
            - tool_runtime
            - safety

            models:
            - model_id: meta-llama/Llama-3.2-1B-Instruct
              provider_id: vllm
              model_type: llm
              provider_model_id: null
            - model_id: sentence-transformers/all-mpnet-base-v2
              metadata:
                  embedding_dimension: 768
              model_type: embedding
              provider_id: sentence-transformers
              provider_model_id: /Users/cpompeia/Documents/Dev/lightspeed-rag-content/embeddings_model
              
            providers:
              inference:
              - provider_id: sentence-transformers
                provider_type: inline::sentence-transformers
                config: {}
              - provider_id: vllm
                provider_type: remote::vllm
                config:
                  url: ${ KSVC_URL }/v1/
                  api_token: ${ VLLM_API_KEY }
                  tls_verify: false
                  max_tokens: 1024

              agents:
              - provider_id: meta-reference
                provider_type: inline::meta-reference
                config:
                  persistence_store:
                    type: sqlite
                    db_path: .llama/distributions/ollama/agents_store.db
                  responses_store:
                    type: sqlite
                    db_path: .llama/distributions/ollama/responses_store.db

              safety:
              - provider_id: llama-guard
                provider_type: inline::llama-guard
                config:
                  excluded_categories: []

              tool_runtime:
              - provider_id: rag-runtime
                provider_type: inline::rag-runtime
                config: {}

            tool_groups:
            - provider_id: rag-runtime
              toolgroup_id: builtin::rag
              args: null
              mcp_endpoint: null

      # Run e2e test
      - name: Run service manually
        run: |          
          docker compose version
          docker compose up -d
          
          # Check for errors and show logs if any services failed
          if docker compose ps | grep -E 'Exit|exited|stopped'; then
            echo "Some services failed to start - showing logs:"
            docker compose logs
            exit 1
          else
            echo "All services started successfully"
          fi

      - name: Wait for services
        run: |
          echo "Waiting for services to be healthy..."
          sleep 20  # adjust depending on boot time

      - name: Quick connectivity test
        run: |
          echo "Testing basic connectivity before full test suite..."
          curl -f http://localhost:8080/v1/models || {
            echo "❌ Basic connectivity failed - showing logs before running full tests"
            docker compose logs --tail=30
            exit 1
          }

      - name: Run e2e tests
        run: |
          echo "Installing test dependencies..."
          pip install uv
          uv sync

          echo "Running comprehensive e2e test suite..."
          make test-e2e

      - name: Destroy OpenShift cluster
        if: always()
        run: |
          openshift-install destroy cluster --dir="$OPENSHIFT_WORKDIR"
